# Audio Context Tool

A privacy-focused, modular pipeline for processing phone call audio and other recordings. Automates ingestion, PII removal, file tracking, audio separation, CLAP annotation, loudness normalization, speaker diarization, transcription, soundbite extraction, LLM integration, remixing, and show creation. All steps are orchestrated for strict privacy, traceability, and manifest/logging requirements.

---

## Major Features & Recent Changes
- **Soundbites folders** are now named after sanitized call titles, not just call IDs.
- **SFW (safe-for-work) call titles** and show summaries are generated and included in outputs.
- **Show notes** are generated by the LLM and appended to show summaries.
- **Extensions system:** Users can add scripts (like `character_persona_builder.py`) to an `extensions/` folder, which run after the main pipeline and can use all outputs.
- **Advanced Character.AI persona generation:** The extension system supports per-channel and per-speaker persona creation, robust to folder naming and batch/single-file workflows.
- **CLAP annotation** is now optional and disabled by default; `out-` files are only processed for CLAP unless `--process-out-files` is set.
- **All outputs, logs, and manifests** are strictly PII-free and fully auditable.
- **Timestamps** in show summaries are now in HH:MM:SS format, with emoji for calls/tones.
- **Resume and force:** The pipeline is robust to resume, with `--resume` and `--resume-from`, and `--force` now archives outputs instead of deleting them.

---

## Features
- **Privacy-first:** No PII in filenames, logs, or outputs. All logging/manifesting is anonymized.
- **Modular pipeline:** Ingestion, separation, CLAP, diarization, normalization, transcription, soundbite, remix, show, LLM, and more.
- **CLAP-based segmentation:** Detects call boundaries in long audio using CLAP (configurable prompts, thresholds).
- **Speaker diarization:** Segments audio by speaker, with per-speaker transcripts and outputs.
- **LLM integration:** Workflow-driven LLM tasks (titles, synopses, categories, image prompts, songs, etc.)
- **Batch processing:** Handles large folders of audio, tuples, or single files (including YouTube/URL inputs).
- **Extensible:** All workflows and prompts are JSON-configurable in the `workflows/` folder.
- **Traceability:** Full manifest and metadata lineage for every file and output.

---

## Directory Structure
```
soundContextTools/
  memory-bank/           # Project docs, context, and rules
  outputs/               # All run outputs (timestamped folders)
  workflows/             # All pipeline, CLAP, and LLM configs (JSON)
  extensions/            # Custom extension scripts (see below)
  ...                    # Pipeline scripts and modules
```

---

## Extensions

You can add custom scripts to the `extensions/` folder. These scripts will be run after the main pipeline completes and can use all outputs (soundbites, transcripts, LLM results, etc.).

### Character Persona Builder Extension

- The `character_persona_builder.py` extension generates advanced Character.AI persona definitions for each call/channel/speaker using transcripts and LLMs.
- **Channel folders may have run-specific prefixes** (e.g., `0000-conversation`). The extension normalizes these for output and is robust to naming.
- **For conversation-only calls:** Generates a separate persona for each detected speaker (no merging).
- **For left/right calls:** Merges all speakers per channel and generates one persona per channel.
- **System prompt and persona style are embedded** for best results (no external files needed).
- **Usage example:**
  ```sh
  python extensions/character_persona_builder.py outputs/run-YYYYMMDD-HHMMSS --llm-config workflows/llm_tasks.json
  ```
- Outputs are written to `characters/<call_title or call_id>/<channel or conversation_speaker>/` with transcript, persona, and audio clips.

### General Extension Best Practices
- Extensions should be robust to folder naming and support both batch and single-file workflows.
- Log only anonymized, PII-free information.
- See `extensions/README.md` for more details and extension authoring tips.

---

## Installation

### 1. Using Conda (Recommended for PyTorch)
```sh
conda create -n soundcontext python=3.10
conda activate soundcontext
# Install PyTorch (choose the right CUDA version for your system):
# See https://pytorch.org/get-started/locally/ for the latest command.
# Example (CPU only):
conda install pytorch torchaudio cpuonly -c pytorch
# Example (CUDA 11.8):
conda install pytorch torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
# Install the rest:
pip install -r requirements.txt
```

### 2. Using Pip Only
- Install the correct torch/torchaudio version for your system from [PyTorch.org](https://pytorch.org/get-started/locally/)
- Then:
```sh
pip install -r requirements.txt
```

---

## Configuration
- **CLAP segmentation:** `workflows/clap_segmentation.json` (prompts, thresholds, pairing logic)
- **CLAP annotation:** `workflows/clap_annotation.json` (prompts, thresholds, chunking)
- **LLM tasks:** `workflows/llm_tasks.json` (task list, prompts, model config)
- **All configs are editable JSON files.**

---

## Usage

### Basic CLI Example
```sh
python pipeline_orchestrator.py <input_dir>
```

### With CLAP-based Call Segmentation
```sh
python pipeline_orchestrator.py <input_dir> --call-cutter
```

### Processing a YouTube/URL Input
```sh
python pipeline_orchestrator.py --url "https://www.youtube.com/watch?v=..."
```

### Resume/Debug
```sh
python pipeline_orchestrator.py --output-folder outputs/run-YYYYMMDD-HHMMSS --resume
```

### Other CLI Options
- `--asr_engine parakeet|whisper` (choose ASR model)
- `--llm_config workflows/llm_tasks.json` (custom LLM task config)
- `--call-tones` (insert tones between calls in show output)
- `--resume-from <stage>` (resume from a specific stage)

---

## Customization
- **CLAP/LLM prompts, thresholds, and logic** are fully tweakable in the `workflows/` JSON files.
- Change prompts, add/remove tasks, adjust thresholds, and rerun the pipelineâ€”no code changes needed.

---

## Troubleshooting
- **PyTorch install issues:** Use conda and follow the [official instructions](https://pytorch.org/get-started/locally/).
- **No segments detected:** Lower the CLAP confidence threshold or add more prompts in `clap_segmentation.json`.
- **Too many/false segments:** Raise the threshold or adjust pairing/gap settings.
- **LLM token limit errors:** The pipeline will chunk transcripts by speaker/segment automatically.
- **Manifest/logs:** Check the output run folder for detailed logs and manifest.json.

---

## Contributing & Support
- PRs and issues welcome!
- For questions, open an issue or contact the maintainer.

---

**Happy hacking!** 